{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd_tokenizer.pt', 'lang': 'de', 'shorthand': 'de_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd_mwt_expander.pt', 'lang': 'de', 'shorthand': 'de_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd_tagger.pt', 'pretrain_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd.pretrain.pt', 'lang': 'de', 'shorthand': 'de_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd_lemmatizer.pt', 'lang': 'de', 'shorthand': 'de_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd_parser.pt', 'pretrain_path': '/Users/sanskritisharma/stanfordnlp_resources/de_gsd_models/de_gsd.pretrain.pt', 'lang': 'de', 'shorthand': 'de_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "[1] Word: Ich\n",
      "Ich, PRON, PPER, nsubj\n",
      "[2] Word: esse\n",
      "esse, VERB, VVFIN, root\n",
      "[3] Word: der\n",
      "der, DET, ART, det\n",
      "[4] Word: Hut\n",
      "Hut, NOUN, NN, obj\n",
      "Masc, Hut\n",
      "Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ReturnWord is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe96d0ecff64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenitive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgovernor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mwords_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s takes the genitive case, so the article should be %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgovernor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this is output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ReturnWord is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "class ReturnWord():\n",
    "    def __init__(self):\n",
    "        self.text = \"\"\n",
    "        self.upos = \"\"\n",
    "        self.xpos = \"\"\n",
    "        self.case = \"\"\n",
    "        self.gender = \"\"\n",
    "        self.number = \"\"\n",
    "        self.governor = \"\"\n",
    "        self.relation = \"\"\n",
    "        self.notes = []\n",
    "        \n",
    "    def set_vars(self, text, upos, xpos, governor, relation):\n",
    "        self.text = text\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos\n",
    "        self.governor = governor\n",
    "        self.relation = relation\n",
    "\n",
    "nlp_de = stanfordnlp.Pipeline(lang='de')\n",
    "#nlp_en = stanfordnlp.Pipeline(lang='en')\n",
    "\n",
    "sentence = \"Ich esse der Hut\"\n",
    "\n",
    "prep_akk = ['bis', 'durch', 'für', 'gegen', 'ohne', 'um']\n",
    "prep_dat = ['aus', 'ausser', 'bei', 'nach', 'mit', 'seit', 'von', 'zu']\n",
    "prep_acc_dat = ['an', 'auf', 'hinter', 'in', 'neben', 'über', 'unter', 'von', 'zwischen']\n",
    "\n",
    "nominative = { 'Masc': 'der', 'Fem': 'die', 'Neut': 'das' }\n",
    "accusative = { 'Masc': 'den', 'Fem': 'die', 'Neut': 'das' }\n",
    "dative = { 'Masc': 'dem', 'Fem': 'der', 'Neut': 'den' }\n",
    "genitive = { 'Masc': 'des', 'Fem': 'der', 'Neut': 'des' }\n",
    "\n",
    "doc_de = nlp_de(sentence)\n",
    "\n",
    "sentences_ret = []\n",
    "\n",
    "for sentence in doc_de.sentences:\n",
    "    words_ret = [ReturnWord() for _ in range(len(sentence.words))]\n",
    "    for word in sentence.words:\n",
    "        print(\"[%d] Word: %s\" % (int(word.index), word.text))\n",
    "        words_ret[int(word.index) - 1].set_vars(word.text, word.upos, word.xpos, word.governor, word.dependency_relation)\n",
    "        print(\"%s, %s, %s, %s\" % (words_ret[int(word.index) - 1].text, words_ret[int(word.index) - 1].upos, words_ret[int(word.index) - 1].xpos, words_ret[int(word.index) - 1].relation))\n",
    "        if(word.upos == 'ADP'):\n",
    "            print(\"%s is ADP\" % word.text)\n",
    "            if(word.lemma in prep_akk):\n",
    "                print(\"%s is accusative\" % word.text)\n",
    "                words_ret[int(word.index) - 1].notes.append(word.text + \" takes the accusative case.\")\n",
    "                words_ret[word.governor - 1].notes.append(\"Because \" + word.text + \" is accusative, so is \" + sentence.words[word.governor - 1].text)\n",
    "                words_ret[word.governor - 1].case = \"Acc\"\n",
    "            elif(word.lemma in prep_dat):\n",
    "                print(\"%s is dative\" % word.text)\n",
    "                words_ret[int(word.index) - 1].notes.append(word.text + \" takes the dative case.\")\n",
    "                words_ret[word.governor - 1].notes.append(\"Because \" + word.text + \" is dative, so is \" + sentence.words[word.governor - 1].text)\n",
    "                words_ret[word.governor - 1].case = \"Dat\"\n",
    "            elif(word.lemma in prep_acc_dat):\n",
    "                print(\"%s is akk/dat\" % word.text)\n",
    "                words_ret[int(word.index) - 1].notes.append(word.text + \" takes Dative if it answers the question 'where?'\")\n",
    "                words_ret[int(word.index) - 1].notes.append(word.text + \" takes the Accusative if it answers the question 'where to?'\")\n",
    "                words_ret[word.governor - 1].notes.append(sentence.words[word.governor - 1].text + \" takes either the Accusative or Dative case.\")\n",
    "                words_ret[word.governor - 1].case = \"Acc|Dat\"\n",
    "        if(word.upos == 'NOUN'):\n",
    "            words_ret[int(word.index) - 1].gender = (nlp_de(word.text).sentences[0].words[0].feats.split('|')[1].split('=')[1])\n",
    "            print(\"%s, %s\" % (words_ret[int(word.index) - 1].gender, word.text))\n",
    "        \n",
    "        feats = word.feats.split('|')\n",
    "        for feat in feats:\n",
    "            pair = feat.split('=')\n",
    "            #print(\"%s: %s\" % (word.text, pair))\n",
    "            if(pair[0] == \"Gender\" and words_ret[int(word.index) - 1].gender == \"\"): words_ret[int(word.index) - 1].gender = pair[1]\n",
    "            elif(pair[0] == \"Number\"): words_ret[int(word.index) - 1].number = pair[1]\n",
    "            elif(pair[0] == \"Case\" and words_ret[int(word.index) - 1].case == \"\"): words_ret[int(word.index) - 1].case = pair[1]\n",
    "            #if(len(pair) is 2): print(\"%s gender: %s (pair[1] is %s)\" % (word.text, words_ret[int(word.index) - 1].gender, pair[1]))\n",
    "        if(word.dependency_relation == 'obj' or word.dependency_relation == 'dobj'):\n",
    "            words_ret[int(word.index) - 1].notes.append(word.text + \" is probably a direct object, which takes the accusative case.\")\n",
    "            words_ret[int(word.index) - 1].case = \"Acc\"\n",
    "        if(word.dependency_relation == 'iobj'):\n",
    "            words_ret[int(word.index) - 1].notes.append(word.text + \" is probably an indirect object, which takes the dative case.\")\n",
    "            words_ret[int(word.index) - 1].case = \"Dat\"\n",
    "            \n",
    "    for word in sentence.words:\n",
    "        #print(\"%s == 'ART' ? %s\" % (word.xpos, (word.xpos == 'ART')))\n",
    "        if(word.xpos == 'ART'):\n",
    "            print(words_ret[word.governor - 1].case)\n",
    "            if('Acc' in words_ret[word.governor - 1].case):\n",
    "                #print(\"%s is Acc %s\" % (str(words_ret[word.governor - 1].text), str(words_ret[word.governor - 1].case)))\n",
    "                nom = nominative[words_ret[word.governor - 1].gender]\n",
    "                article = accusative[words_ret[word.governor - 1].gender]\n",
    "                words_ret[int(word.index) - 1].notes.append(\"%s %s takes the accusative case, so the article should be %s\" % (nom, words_ret[word.governor - 1].text, article))\n",
    "            if('Dat' in words_ret[word.governor - 1].case):\n",
    "                #print(\"%s is Dat %s\" % (str(words_ret[word.governor - 1].text), str(words_ret[word.governor - 1].case)))\n",
    "                nom = nominative[words_ret[word.governor - 1].gender]\n",
    "                article = dative[words_ret[word.governor - 1].gender]\n",
    "                words_ret[int(word.index) - 1].notes.append(\"%s %s takes the dative case, so the article should be %s\" % (nom, words_ret[word.governor - 1].text, article))\n",
    "            if('Gen' in words_ret[word.governor - 1].case):\n",
    "                #print(\"%s is Gen %s\" % (str(words_ret[word.governor - 1].text), str(words_ret[word.governor - 1].case)))\n",
    "                nom = nominative[words_ret[word.governor - 1].gender]\n",
    "                article = genitive[words_ret[word.governor - 1].gender]\n",
    "                words_ret[int(word.index) - 1].notes.append(\"%s %s takes the genitive case, so the article should be %s\" % (nom, words_ret[word.governor - 1].text, article))\n",
    "print(\"this is output\",json.dumps(words_ret[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'version' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-518c6893639c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'version' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
